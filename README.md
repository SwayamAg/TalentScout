# TalentScout Hiring Assistant ğŸ¤–

An AI-powered hiring assistant chatbot built using **Streamlit** and **Azure OpenAI (GPT-4o-mini)** to automate initial candidate screening through structured conversations and dynamic technical question generation.

---

## ğŸ“Œ Project Overview

TalentScout is designed to assist recruitment teams by conducting **initial candidate screening**.  
The chatbot collects essential candidate details, understands the candidateâ€™s declared tech stack, and generates **relevant technical interview questions** tailored to that stack.

The system focuses on:
- Prompt engineering
- Context-aware interaction
- Privacy-first data handling
- Clean, explainable architecture

---

## ğŸ¯ Key Features

- **Interactive Streamlit Chat UI**
- **Step-by-step candidate information gathering**
- **Tech stackâ€“based technical question generation (3â€“5 questions)**
- **Context-aware conversation flow**
- **Graceful exit handling**
- **Session-only, in-memory data handling**

---

## ğŸ¥ Project Preview (Demo)

> ğŸ“Œ **Demo Video:**  
> [Watch the demo video on Google Drive](https://drive.google.com/file/d/1oA0PWhOm9C4mDXy1nSvc1_Wm5ZyUKnqe/view?usp=sharing)

---

## ğŸ§  Prompt Engineering Strategy

The project uses carefully designed prompts to:

1. **Define the chatbot role**  
   A system prompt restricts the chatbot strictly to hiring and technical screening.

2. **Guide technical question generation**  
   A structured prompt template ensures:
   - Stack-specific questions
   - No answers or hints
   - Appropriate difficulty level

3. **Maintain coherent conversation flow**  
   Prompts and application logic work together to ensure the chatbot does not deviate from its purpose.

---

## ğŸ—ï¸ System Architecture

```
â”œâ”€â”€ app.py              # Streamlit UI and conversation flow
â”œâ”€â”€ llm_client.py       # Azure OpenAI (GPT-4o-mini) client wrapper
â”œâ”€â”€ conversation.py     # Conversation logic and helpers
â”œâ”€â”€ prompts.py          # System and task-specific prompts
â”œâ”€â”€ requirements.txt    # Project dependencies
â”œâ”€â”€ .env                # Environment variables (not committed)
```

---

## âš™ï¸ Working of the Project

### **Conversation Flow & State Management**

The application uses a **step-based state machine** to manage the conversation flow through Streamlit's `st.session_state`. Here's how it works:

#### **Step 0: Initialization**
- When the app loads for the first time, `step = 0`
- The chatbot displays a greeting message (`get_greeting_message()`)
- Immediately asks for the first field: **"Full Name"**
- Sets `step = 1` and initializes `field_index = 0`

#### **Step 1: Candidate Information Collection**
- The app collects **6 fields sequentially** using `field_index` to track progress:
  1. Full Name
  2. Email Address
  3. Phone Number
  4. Years of Experience
  5. Desired Position(s)
  6. Current Location
- For each user input:
  - Stores the value in `candidate_data[field_name]`
  - Increments `field_index`
  - If more fields remain â†’ asks for the next field
  - If all fields collected â†’ moves to `step = 2` and asks for tech stack

#### **Step 2: Tech Stack & Question Generation**
- User provides their tech stack (e.g., "Python, Django, PostgreSQL, React")
- The app:
  1. Stores tech stack in `candidate_data["Tech Stack"]`
  2. Displays: *"Generating technical questions based on your tech stack..."*
  3. Calls `generate_technical_questions(tech_stack)` from `conversation.py`
  4. This function:
     - Formats `TECH_QUESTION_PROMPT_TEMPLATE` with the tech stack
     - Creates a messages array with `SYSTEM_PROMPT` and the formatted prompt
     - Calls `get_llm_response()` from `llm_client.py`
     - Returns 3â€“5 technical questions from Azure OpenAI
  5. Displays the generated questions
  6. Shows closing message
  7. Sets `step = 3` (conversation ends)

#### **Step 3: Conversation Complete**
- No further input is processed
- All collected data remains in `st.session_state.candidate_data` (in-memory only)

### **Component Interactions**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   app.py    â”‚  â† Main UI & State Management
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â†’ conversation.py
       â”‚      â”œâ”€â”€â†’ get_greeting_message()
       â”‚      â”œâ”€â”€â†’ get_closing_message()
       â”‚      â”œâ”€â”€â†’ is_exit_message()
       â”‚      â””â”€â”€â†’ generate_technical_questions()
       â”‚              â”‚
       â”‚              â””â”€â”€â†’ llm_client.py
       â”‚                     â””â”€â”€â†’ get_llm_response()
       â”‚                             â”‚
       â”‚                             â””â”€â”€â†’ Azure OpenAI API
       â”‚
       â””â”€â”€â†’ prompts.py
              â”œâ”€â”€â†’ SYSTEM_PROMPT
              â””â”€â”€â†’ TECH_QUESTION_PROMPT_TEMPLATE
```

### **Key Mechanisms**

1. **State Persistence**
   - `st.session_state.messages`: Stores entire chat history
   - `st.session_state.step`: Tracks conversation phase (0, 1, 2, 3)
   - `st.session_state.candidate_data`: Dictionary of collected information
   - `st.session_state.field_index`: Tracks which field is being collected

2. **Rerun Behavior**
   - Streamlit reruns the entire script on each user interaction
   - `st.rerun()` is called after processing user input to refresh the UI
   - State variables persist across reruns, maintaining conversation context

3. **Exit Handling**
   - User can type: `exit`, `quit`, `done`, or `thank you` (case-insensitive, exact match)
   - Triggers `is_exit_message()` â†’ displays closing message â†’ calls `st.stop()`

4. **LLM Integration**
   - `llm_client.py` initializes `AzureOpenAI` client using environment variables
   - `get_llm_response()` uses temperature `0.3` for consistent, deterministic output
   - Model deployment name is read from `AZURE_OPENAI_DEPLOYMENT_NAME`

5. **Prompt Engineering**
   - `SYSTEM_PROMPT` defines the chatbot's role and constraints
   - `TECH_QUESTION_PROMPT_TEMPLATE` is dynamically filled with candidate's tech stack
   - Prompts ensure no answers are provided, only questions

### **Data Flow Example**

```
User Input: "John Doe"
    â†“
app.py: Stores in candidate_data["Full Name"]
    â†“
app.py: Increments field_index, asks for Email
    â†“
User Input: "john@example.com"
    â†“
... (continues for all 6 fields)
    â†“
User Input: "Python, Django, PostgreSQL"
    â†“
app.py: Calls generate_technical_questions("Python, Django, PostgreSQL")
    â†“
conversation.py: Formats prompt with tech stack
    â†“
llm_client.py: Sends request to Azure OpenAI
    â†“
Azure OpenAI: Returns 3-5 technical questions
    â†“
app.py: Displays questions + closing message
```

---

## ğŸ” Data Handling & Privacy

- Candidate data is stored **only in memory** using `st.session_state`
- No databases, files, or logs are used to persist data
- Data is automatically cleared when the session ends or the page is refreshed
- This approach aligns with **privacy-by-design** principles and GDPR-friendly practices for demo systems

---

## âš™ï¸ Tech Stack

- **Frontend:** Streamlit
- **Backend:** Python
- **LLM:** Azure OpenAI (GPT-4o-mini)
- **Environment Management:** python-dotenv

---

## ğŸš€ How to Run Locally

### 1ï¸âƒ£ Clone the repository
```bash
git clone <your-repo-url>
cd TalentScout
```

### 2ï¸âƒ£ Create a virtual environment
```bash
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
```

### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Set up environment variables
Create a `.env` file:
```env
AZURE_OPENAI_ENDPOINT=https://<your-resource>.openai.azure.com
AZURE_OPENAI_API_KEY=your_api_key
AZURE_OPENAI_DEPLOYMENT_NAME=your_deployment_name
AZURE_OPENAI_API_VERSION=2024-02-15-preview
```

### 5ï¸âƒ£ Run the application
```bash
streamlit run app.py
```

---

## ğŸ§ª Example Flow

1. Chatbot greets the candidate and explains its purpose  
2. Collects candidate details:
   - Name, Email, Phone, Experience, Desired Role, Location
3. Prompts for tech stack
4. Generates 3â€“5 tailored technical questions
5. Ends the conversation gracefully

---

## âš ï¸ Challenges & Solutions

### 1ï¸âƒ£ Managing conversation flow in Streamlit
Streamlit reruns the script on every user interaction. This was addressed using `st.session_state` along with step-based and index-based tracking to preserve conversation context and ensure a coherent, sequential flow.

### 2ï¸âƒ£ Azure OpenAI integration
Azure OpenAI requires deployment-based routing and explicit API versioning. The project uses the `AzureOpenAI` client with a fixed API version to ensure stable and correct communication with the GPT-4o-mini model.

### 3ï¸âƒ£ Data privacy considerations
To avoid unnecessary storage of sensitive information, all candidate data is handled in-memory only and is automatically cleared when the session ends or the page is refreshed, ensuring privacy-first behavior.

---

## ğŸ”® Future Enhancements

- Follow-up and adaptive questioning based on candidate responses
- Candidate proficiency scoring
- Integration with databases or ATS systems
- Use of **LangChain** for advanced prompt chaining and conversational memory
- Multilingual support

---

## ğŸ’³ Credits

- Built with â¤ï¸ by [Swayam Agarwal](https://github.com/SwayamAg)
- Uses [Streamlit](https://streamlit.io/), [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service), and [Python](https://www.python.org/)

---

## ğŸ“„ License

### *Usage Terms*

- This project is provided as-is for learning and demonstration purposes
- Not intended for production use without proper security, error handling, and compliance measures
- Candidate data is handled in-memory only and is not persisted (see [Data Handling & Privacy](#-data-handling--privacy))
- Users are responsible for ensuring compliance with data protection regulations (GDPR, CCPA, etc.) if deploying

### *Attribution*

If you use this project as a reference or base for your own work, please provide appropriate attribution to the original project.
